# ecommerce_scraper/spiders/laptops.py
import scrapy
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time

class LaptopsSpider(scrapy.Spider):
    name = 'laptops'
    
    custom_settings = {
        'DOWNLOAD_DELAY': 2,
        'CONCURRENT_REQUESTS': 1,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
        'HTTPCACHE_ENABLED': False,
        'FEED_EXPORT_ENCODING': 'utf-8',
    }
    
    def __init__(self):
        print("üöÄ Initialisation du spider avec Selenium...")
        
        # Configuration Chrome
        chrome_options = Options()
        
        # ‚≠ê IMPORTANT: Sp√©cifier le chemin de Chrome.exe
        # Chemins courants pour Chrome:
        chrome_paths = [
            r"C:\Program Files\Google\Chrome\Application\chrome.exe",  # Standard
            r"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe",  # 32-bit
            r"C:\Users\MARCOM\AppData\Local\Google\Chrome\Application\chrome.exe",  # User install
        ]
        
        chrome_binary = None
        import os
        for path in chrome_paths:
            if os.path.exists(path):
                chrome_binary = path
                print(f"‚úÖ Chrome trouv√© √†: {path}")
                break
        
        if chrome_binary:
            chrome_options.binary_location = chrome_binary
        else:
            print("‚ö†Ô∏è Chrome non trouv√© aux emplacements standards")
            print("üí° Veuillez installer Chrome ou sp√©cifier le chemin manuellement")
        
        # chrome_options.add_argument("--headless")  # ‚ö†Ô∏è Comment√© pour mode visible
        chrome_options.add_argument("--start-maximized")  # Fen√™tre maximis√©e
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # ‚≠ê Chemin vers chromedriver
        chromedriver_path = r"C:\chromedriver\chromedriver.exe"  # üìå √Ä ADAPTER
        
        try:
            self.driver = webdriver.Chrome(
                service=Service(chromedriver_path),
                options=chrome_options
            )
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.wait = WebDriverWait(self.driver, 20)
            print("‚úÖ Driver Chrome initialis√© avec succ√®s!")
        except Exception as e:
            print(f"‚ùå Erreur lors de l'initialisation du driver: {e}")
            print("\nüí° SOLUTION:")
            print("   1. V√©rifiez votre version de Chrome: chrome://version/")
            print("   2. T√©l√©chargez ChromeDriver: https://googlechromelabs.github.io/chrome-for-testing/")
            print(f"   3. Placez chromedriver.exe √†: {chromedriver_path}")
            raise
    
    def start_requests(self):
        """Point d'entr√©e du spider - on commence par la page 1"""
        url = "https://webscraper.io/test-sites/e-commerce/ajax/computers/laptops"
        print(f"\n{'='*70}")
        print(f"üîÑ D√âBUT DU SCRAPING MULTI-PAGES")
        print(f"{'='*70}\n")
        yield scrapy.Request(url, callback=self.parse_all_pages, dont_filter=True)
    
    def parse_all_pages(self, response):
        """
        Scrape toutes les pages en utilisant Selenium pour la navigation
        (Approche inspir√©e de votre code Selenium qui fonctionne)
        """
        current_page = 1
        max_pages = 20
        
        # Charger la premi√®re page
        self.driver.get(response.url)
        print(f"üì° Navigation vers: {response.url}")
        
        # Attendre que les produits se chargent
        try:
            self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, "thumbnail")))
            print("‚úÖ Page initiale charg√©e avec succ√®s!")
            time.sleep(3)
        except TimeoutException:
            print("‚ùå Timeout: impossible de charger la page")
            return
        
        # Boucle de scraping multi-pages
        while current_page <= max_pages:
            try:
                print(f"\nüìÑ PAGE {current_page}")
                print("-" * 70)
                
                # Attendre que les produits soient pr√©sents
                self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, "thumbnail")))
                time.sleep(2)
                
                # R√©cup√©rer tous les produits de la page
                products = self.driver.find_elements(By.CLASS_NAME, "thumbnail")
                print(f"   üîç {len(products)} ordinateurs trouv√©s")
                
                if len(products) == 0:
                    print("   ‚ö†Ô∏è Aucun produit - Arr√™t du scraping")
                    break
                
                # Scraper chaque produit
                for idx, product in enumerate(products, 1):
                    try:
                        # Scroller vers le produit
                        self.driver.execute_script(
                            "arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", 
                            product
                        )
                        time.sleep(0.15)
                        
                        # Extraction des donn√©es
                        title = product.find_element(By.CLASS_NAME, "title").text.strip()
                        price_text = product.find_element(By.CLASS_NAME, "price").text
                        price = price_text.replace("$", "").replace(",", "").strip()
                        
                        try:
                            description = product.find_element(By.CLASS_NAME, "description").text.strip()
                        except NoSuchElementException:
                            description = ""
                        
                        try:
                            reviews_text = product.find_element(By.CSS_SELECTOR, ".ratings p.review-count").text.strip()
                            reviews = reviews_text.split()[0] if reviews_text else "0"
                        except NoSuchElementException:
                            reviews = "0"
                        
                        # Extraction du rating (nombre d'√©toiles)
                        rating = self.extract_rating(product)
                        
                        try:
                            link = product.find_element(By.CLASS_NAME, "title").get_attribute("href")
                        except NoSuchElementException:
                            link = ""
                        
                        # Yield du r√©sultat
                        yield {
                            'page': current_page,
                            'title': title,
                            'price': price,
                            'description': description,
                            'reviews': reviews,
                            'rating': rating,
                            'link': link
                        }
                        
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è Erreur produit #{idx}: {str(e)[:50]}")
                        continue
                
                print(f"   ‚úÖ Page {current_page} scrap√©e avec succ√®s!")
                
                # Tenter de passer √† la page suivante
                if current_page < max_pages:
                    print(f"\n   üîÑ Tentative de navigation vers page {current_page + 1}...")
                    
                    if not self.click_next_button():
                        print(f"\n   ‚ÑπÔ∏è Fin de la pagination d√©tect√©e √† la page {current_page}")
                        break
                    
                    current_page += 1
                    
                    print(f"   ‚è≥ Chargement de la page {current_page}...")
                    time.sleep(4)
                    
                    # Scroller pour stabiliser
                    self.driver.execute_script("window.scrollTo(0, 0);")
                    time.sleep(0.5)
                    self.driver.execute_script("window.scrollTo(0, 800);")
                    time.sleep(1)
                    
                    try:
                        self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, "thumbnail")))
                        print(f"   ‚úÖ Page {current_page} charg√©e avec succ√®s!")
                    except TimeoutException:
                        print("   ‚ö†Ô∏è Timeout - Nouveaux produits non charg√©s")
                        break
                else:
                    break
                    
            except Exception as e:
                print(f"   ‚ùå Erreur critique sur page {current_page}: {str(e)[:100]}")
                break
        
        print(f"\n{'='*70}")
        print(f"üéâ SCRAPING TERMIN√â!")
        print(f"üìÑ Nombre de pages parcourues: {current_page}")
        print(f"{'='*70}\n")
    
    def extract_rating(self, product):
        """
        Extrait le nombre d'√©toiles (rating) d'un produit
        Le site utilise ws-icon ws-icon-star pour les √©toiles
        """
        try:
            rating_stars = product.find_elements(By.CSS_SELECTOR, ".ratings .ws-icon-star")
            rating = len(rating_stars)
            if rating > 0:
                return rating
        except:
            pass
        return 0
    
    def click_next_button(self):
        """
        Clique sur le bouton 'Next >' pour passer √† la page suivante
        Retourne True si le clic a r√©ussi, False sinon
        """
        try:
            # Scroller vers le bas
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1.5)
            
            print("   üîç Recherche du bouton 'Next >'...")
            
            # Trouver tous les boutons
            all_buttons = self.driver.find_elements(By.TAG_NAME, "button")
            
            for button in all_buttons:
                text = button.text.strip()
                
                if "next" in text.lower() or text == ">" or ">" in text:
                    print(f"      Bouton trouv√©: '{text}'")
                    
                    # V√©rifier si le bouton est d√©sactiv√©
                    if not button.is_enabled() or button.get_attribute("disabled"):
                        print("   üèÅ Bouton d√©sactiv√© - Derni√®re page atteinte!")
                        return False
                    
                    # Scroller vers le bouton et cliquer
                    self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", button)
                    time.sleep(0.5)
                    self.driver.execute_script("arguments[0].click();", button)
                    print(f"   ‚úÖ Clic r√©ussi sur le bouton '{text}'!")
                    return True
            
            print("   ‚ö†Ô∏è Aucun bouton 'Next' trouv√©")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur lors du clic: {str(e)[:80]}")
        
        return False
    
    def closed(self, reason):
        """Fermeture propre du driver Selenium"""
        print("\n‚è≥ Fermeture du navigateur...")
        self.driver.quit()
        print("‚úÖ Navigateur ferm√©. Spider termin√© avec succ√®s!")